---
title: "Building Your First CHAP Model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Building Your First CHAP Model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This tutorial walks you through building a CHAP-compatible model step-by-step, using a validation-first approach to ensure your model works correctly before deploying it.

## Setup

```{r setup}
library(chap.r.sdk)
library(dplyr)
```

## Step 1: Understand the Function Interface

Every CHAP model requires two functions:

**Training function:**
```r
train_fn <- function(training_data, model_configuration = list()) {
  # training_data: tsibble with time_period index, location key,
  #                disease_cases, and covariates
  # model_configuration: optional list of parameters
  # Returns: any model object (list, fitted model, etc.)
}
```

**Prediction function:**
```r
predict_fn <- function(historic_data, future_data, saved_model,
                       model_configuration = list()) {
  # historic_data: tsibble with historical observations
  # future_data: tsibble with time periods to predict
  # saved_model: the object returned by train_fn
  # Returns: tibble with samples list-column containing numeric vectors
  #   - For deterministic models: single sample per row (e.g., samples = list(c(42)))
  #   - For probabilistic models: multiple samples per row (e.g., 1000 samples)
  #
  # IMPORTANT: historic_data may contain more recent data than training_data.
  # For time series models, you should refit to historic_data before forecasting.
  # Use saved_model for hyperparameters/structure, not the fitted model itself.
  # See examples/arima_model/ for a demonstration of this pattern.
}
```

## Step 2: Explore the Example Data

The SDK provides example datasets for testing. Let's examine the Laos monthly data:

```{r explore-data}
data <- get_example_data('laos', 'M')
names(data)
```

The example data contains four tsibbles. Each has `time_period` as the index and `location` as the key:

**Training data** - what your model learns from:
```{r}
data$training_data
```

**Historic data** - observations available at prediction time:
```{r}
data$historic_data
```

Notice that `historic_data` extends beyond `training_data`:

```{r}
# Training data ends at:
max(data$training_data$time_period)

# Historic data ends at:
max(data$historic_data$time_period)
```

This is a key concept: when CHAP calls your prediction function, `historic_data` may contain **more recent observations** than what the model was trained on. For time series models (ARIMA, exponential smoothing, etc.), you should **refit** the model to `historic_data` before forecasting. See `examples/arima_model/` for a demonstration of this pattern.

**Future data** - time periods to predict (no `disease_cases`):
```{r}
data$future_data
```

**Example predictions** - what your model should output:
```{r}
data$predictions
```

The predictions tibble has a `samples` list-column where each element is a numeric vector. Let's look at the structure:

```{r}
# Each row has a vector of samples
data$predictions$samples[[1]]
```

For probabilistic models, each vector contains multiple Monte Carlo samples (e.g., 1000). For deterministic models, use a single sample per row: `samples = list(c(42))`.

## Step 3: Validate Before Implementing

Before writing any model logic, let's see what the validation expects. Start with stub functions:

```{r validate-stubs}
train_fn <- function(training_data, model_configuration = list()) {
  list(dummy = 1)
}

predict_fn <- function(historic_data, future_data, saved_model,
                       model_configuration = list()) {
  future_data
}

result <- validate_model_io(train_fn, predict_fn, data)
result$success
result$errors
```

The validation tells us exactly what's missing: the `samples` list-column in predictions.

## Step 4: Implement a Simple Mean Model

Now let's implement a minimal model that predicts the historical mean for each location.
Since all models must return a `samples` list-column, we wrap the single prediction value in a list:

```{r implement-model}
train_fn <- function(training_data, model_configuration = list()) {
  means <- training_data |>
    as_tibble() |>
    summarise(mean_cases = mean(disease_cases, na.rm = TRUE), .by = location)
  list(means = means)
}

predict_fn <- function(historic_data, future_data, saved_model,
                       model_configuration = list()) {
  future_data |>
    left_join(saved_model$means, by = "location") |>
    mutate(samples = purrr::map(mean_cases, ~c(.x))) |>
    select(-mean_cases)
}
```

Note: We use `as_tibble()` in the training function because `summarise(.by = ...)` needs a tibble to collapse across the time dimension. The prediction function works directly on tsibbles since `left_join()` and `mutate()` preserve tsibble structure.

## Step 5: Validate the Implementation

```{r validate-implementation}
result <- validate_model_io(train_fn, predict_fn, data)
result$success
result$n_predictions
```

The validation passes and we generated 21 predictions.

## Step 6: Validate Against All Datasets

The SDK can validate against all available example datasets:

```{r validate-all}
result <- validate_model_io_all(train_fn, predict_fn)
result$success
names(result$results)
```

## Step 7: Create the CLI

Once validation passes, wrap your model in a CLI. Create a file called `model.R`:

```r
library(chap.r.sdk)
library(dplyr)

train_fn <- function(training_data, model_configuration = list()) {
  means <- training_data |>
    as_tibble() |>
    summarise(mean_cases = mean(disease_cases, na.rm = TRUE), .by = location)
  list(means = means)
}

predict_fn <- function(historic_data, future_data, saved_model,
                       model_configuration = list()) {
  future_data |>
    left_join(saved_model$means, by = "location") |>
    mutate(samples = purrr::map(mean_cases, ~c(.x))) |>
    select(-mean_cases)
}

if (!interactive()) {
  create_chap_cli(train_fn, predict_fn)
}
```

## Step 8: Use the CLI

Your model is now ready for command-line use:
```bash
# Train the model
Rscript model.R train training_data.csv

# Generate predictions
Rscript model.R predict historic.csv future.csv model.rds

# Display model info
Rscript model.R info
```

The CLI automatically handles:

- Loading CSV files
- Converting to tsibbles
- Saving the model as RDS
- Writing predictions to CSV (converting nested samples to wide format)

## Probabilistic Models

For probabilistic forecasting, include multiple Monte Carlo samples instead of a single value:

```r
predict_fn <- function(historic_data, future_data, saved_model,
                       model_configuration = list()) {
  n_samples <- 1000

  future_data |>
    left_join(saved_model$means, by = "location") |>
    rowwise() |>
    mutate(
      # Generate 1000 samples from Poisson distribution
      samples = list(rpois(n_samples, lambda = mean_cases))
    ) |>
    ungroup() |>
    select(-mean_cases)
}
```

The `samples` column is a list-column where each element is a numeric vector.
The CLI automatically converts this to wide CSV format (`sample_0`, `sample_1`, ...) for CHAP.

## Working with Samples

The SDK provides utility functions for working with sample-based predictions:

```r
# Convert nested samples to wide format
wide_preds <- predictions_to_wide(nested_preds)

# Convert to long format for scoringutils
long_preds <- predictions_to_long(nested_preds)

# Compute quantiles for hub submissions
quantile_preds <- predictions_to_quantiles(nested_preds)

# Add summary statistics (mean, median, CIs)
preds_with_summary <- predictions_summary(nested_preds)
```

## Summary

The development workflow is:

1. **Explore** example data with `get_example_data()`
2. **Validate** with stubs using `validate_model_io()` to understand requirements
3. **Implement** your train and predict functions
4. **Validate** the implementation
5. **Test** against all datasets with `validate_model_io_all()`
6. **Deploy** with `create_chap_cli()`

## Next Steps

- See `examples/ewars_model/` for a more complex example with configuration
- Read about configuration schemas in `?create_config_schema`
- Explore spatial-temporal utilities in `?aggregate_temporal`
